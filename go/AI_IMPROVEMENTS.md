# AI优化改进说明

## 问题分析

用户在使用 `-optimized` 参数时发现AI棋力不足，作为先手方竟然输给了人类玩家。分析对局记录发现：

1. **搜索深度不足**：4层搜索在复杂中局阶段分析不够深入
2. **候选着法过少**：12个候选可能错过关键战术机会  
3. **评估参数保守**：威胁识别和防御评估不够敏锐
4. **过早终止搜索**：为追求速度牺牲了战术准确性

## 改进方案

### 1. 增强战术意识
- **候选着法**：从12个增加到16个，减少遗漏重要走法
- **威胁检测**：新增 `hasComplexThreats()` 识别多重活三、混合威胁
- **活三计数**：实现 `countLiveThreats()` 精确统计战术威胁

### 2. 智能自适应搜索
- **自适应深度**：复杂局面自动加深搜索至6层
- **战术检测**：识别到复杂威胁时强制深搜
- **局面分析**：根据棋局阶段调整搜索策略

### 3. 改进评估参数
```go
LiveFour:      380000 → 更高的获胜优先级
DeadFourA:     300000 → 更强的威胁检测  
LiveThreeNear: 2200   → 增强的活三评估
ThreeInRowVariants: {
    "open":   30000   → 更重视开放性活三
    "closed": 35000   → 更好的封闭活三评估
}
```

### 4. 平衡性能优化
- **智能终止**：只在确实强势时提前结束（>900000）
- **缓存机制**：增加评估缓存提高重复计算效率
- **剪枝优化**：改进alpha-beta剪枝算法

## 技术特点

### 保持偶数深度
所有搜索深度都保持偶数（2,4,6），确保minimax算法在MIN节点结束，提供更保守可靠的评估。

### 复杂威胁检测
```go
func hasComplexThreats() bool {
    // 检测多个活三（潜在双重威胁）
    // 检测混合威胁（活三+活四组合）
    // 复杂局面触发深度搜索
}
```

### 智能候选选择
```go
func getImprovedCandidateLimit() int {
    // 战术复杂时增加候选数量
    // 深度搜索时适度减少候选
    // 开局阶段增加探索范围
}
```

## 预期效果

1. **速度**：保持快速响应（<0.1秒/步）
2. **棋力**：显著提升复杂局面的战术分析能力
3. **平衡**：在性能和准确性之间找到更好平衡点
4. **稳定**：偶数深度确保minimax算法正确性

## 使用方法

```bash
# 使用改进的优化AI
./gobang -optimized

# 对比测试
./gobang           # 原始AI（最强但较慢）
./gobang -balanced # 平衡AI（中等速度和强度）
./gobang -enhanced # 增强AI（6层深度+算法优化）
```

改进后的优化AI应该能在保持快速响应的同时，在您提到的复杂战术局面中表现显著更佳。